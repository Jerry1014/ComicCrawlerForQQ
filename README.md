# 腾讯动漫爬虫
- 爬取腾讯动漫上的漫画
- 环境：py3    第三方库：selenium，requests

# 太监预告！！！
       surface + bilibili漫画uwp，体验满分，强烈推荐大家使用
       
       再加上没什么人用，以后就缓慢更新

## 食用方法
- 1.下载py脚本
    - 原脚本后缀名为pyw(无窗口)，可改为py以获得输出信息
- 2.如果你有chrome浏览器的话，仅需下载chromedriver.exe
- 3.~~否则，根据你的电脑下载对应的IEDriverServer(32/64).exe~~ （在将来支持）
- **注意：XXX（chromedriver）.exe必须和脚本放在同一目录下**
- 4.通过命令行打开脚本
  - 参数选项：
  
            第一个参数
            1.无 将按配置文件的默认设置进行爬取，无配置文件报错 无后接详细配置参数
            2. m (modify file) 在配置文件的基础上进行修改（永久性） 后接可选的详细配置参数，见下
            3. t (temporary) 在配置文件的基础上，进行临时性的爬取  后接可选的详细配置参数，见下
            4. s (set file) 建立一个配置文件，若之前已经存在，则会被覆盖，修改参数也可通过直接修改配置文件完成  后接必选的详细配置参数，见下
            5. h 输出帮助文档
            
            具体的参数配置 mf/t 下列参数均为可选参数 s则必须包含下列所有参数（除-e外）
            -c xxx 如 -c 505430/  将要爬取的漫画设置为海贼王 从你要爬取的漫画的url中取得
            -se xxx 如 -se 945  将开始爬取的话数设定为第945话 ！！！（不包括第945话）！！！
            -n xxx 如 -n 5  从设置的开始爬取话数开始，一共爬取5话（如有更新），默认为4，仅对本次有效，不记录到配置文件
            -p xxx 如 -p D:\\tem\\  将漫画图片的保存路径设置为D:\\tem\\
            -e xxx 如 -e 13322468550@163.com  设置将爬取结果以邮件方式发送的发送者邮箱  ！！！此参数后必须带有以下的参数 -r xxx 爬取结果的收件人邮箱 -psw xxx 发送邮箱的密码
            
            示例：
            脚本名.pyw s -c 505430/ -se 945 -n 5 -p D:\tem\ -e 133xxxx8550@163.com -r 757xxx393@qq.com -psw xxx
            第一次使用脚本或需要生成新的配置文件 爬取海贼王 从945集开始 连续爬取5话 爬取的图片保存在D:\tem\下 最后的爬取结果通过133xxxx8550@163.com发送到757xxx383@qq。com 其中发送邮箱的密码是xxx
            
            *由于本人相当懒，暂时不打算做选项的输入检测

## 使用说明
- 第一次使用，必须通过s参数来增加一个配置文件，且必须指定所有的可选参数，具体参数设置见上，示例见下
        
      脚本名.pyw s -c 505430/ -se 945 -n 5 -p D:\tem\ -e 133xxxx8550@163.com -r 757xxx393@qq.com -psw xxx
      第一次使用脚本或需要生成新的配置文件 爬取海贼王 从945集开始 连续爬取5话 爬取的图片保存在D:\tem\下 最后的爬取结果通过133xxxx8550@163.com发送到757xxx383@qq。com 其中发送邮箱的密码是xxx
- 配置文件保存至在当前工作目录，命名为setting.csv，可直接修改配置文件，可修改脚本中的DEFAULT_FILE_NAME修改文件命名。
- 理论上来说可以爬取所有的动漫，但本人只测试了海贼王，如有不兼容的漫画，请提issue，作者很闲。
- 在爬取得到图片后，建议使用Kindle Comic Converter生成mobi漫画，在kindle上看会更舒服哦。
- 爬取结果将通过邮件发送到指定邮箱 （配置文件）。

## 碎碎念（特性/介绍）
- 使用headless chrome + selenium爬取
- 一共会启动三个进程，主进程在自己爬取漫画之前，还负责任务的分配，每个进程对图片的爬取则会用多线程
- 爬虫会将结束的最大集数写入配置文件，下次运行爬虫时，可以选择按照最后爬取的集数再往下爬取4集  （可通过命令参数指定一临时量或在脚本修改DEFAULT_EACH_NUM参数）
- 可用于计划任务

## 等哪天我不再摸鱼的时候
- 重构和优化，加快爬取速度
- 消灭小黑窗
- 直接执行js甚至破解图片url的加密，抛弃selenium，从而实现更快速地爬取